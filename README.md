# Lip Recognizer
A project to recognize the video or streaming video's lip 


### Related Papers
| year | org | paper  | sources |
| ---- | --- | -----  | ------- |
| 2021 | Oxford | [Sub-word Level Lip Reading With Visual Attention](https://arxiv.org/pdf/2110.07603v2.pdf) | [official implementation](https://github.com/prajwalkr/vtp) |
| 2016 | Deepmind | [LipNet: End-to-End Sentence-level Lipreading](https://arxiv.org/pdf/1611.01599.pdf) | [LipNet-PyTorch](https://github.com/VIPL-Audio-Visual-Speech-Understanding/LipNet-PyTorch)| 

### Related Tools
| Tool Name | Intros |
| --------- | ------ |
| [Visual Speech Recognition for Multiple Languages](https://github.com/mpc001/Visual_Speech_Recognition_for_Multiple_Languages#introduction)| Composed of face tracking, pre-processing and acoustic/visual encoder and support multiple language| 
### Related Datasets
| Year | Dataset Name | Intros | Languages | 
|----- | ----- |-----| ----- |
|2018 | [Lip Reading Sentences 2 (LRS2)](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html) [1] | Thousands  of spoken sentences from `BBC television` .| English  |
| 2018 | [Lip Reading Sentences 3 (LRS3)](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html) [2] | The dataset consists of thousands of spoken sentences from `TED and TEDx videos` | Unknown |
|2019| [Chinese Mandarin Lip Reading (CMLR)](https://www.vipazoo.cn/CMLR.html) [3]| The dataset consists of 102,072 spoken sentences from 11 speakers| Chinese |
|2020 | [CMU Multimodal Opinion Sentiment, Emotions and Attributes (CMU-MOSEAS)](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/) [4] |  The dataset contains more than 23,500 sentence utterance videos from more than 1000 online YouTube speakers. | Unknown|
| 2006 | [GRID](https://zenodo.org/record/3625687) [5] | Consists of high-quality audio and video (facial) recordings of 1000 sentences spoken by each of 34 talkers | English| 
| 2018 | [Lombard GRID](https://spandh.dcs.shef.ac.uk//avlombard/)[6]|Includes 54 talkers, with 100 utterances per talker (50 Lombard and 50 plain utterances| English| 
|2015 | [TCD-TIMIT] [7]| | |

### Related Companies
| Name | Intros | 
| ---- | ------ |
| [Liopa](https://liopa.ai/) | Provide AI-based lipread technologies.|

### Reference

[1] Afouras, T., Chung, J. S., Senior, A., Vinyals, O. & Zisserman, A. Deep audio-visual speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2018).

[2] Afouras, T., Chung, J. S. & Zisserman, A. LRS3-TED: a large-scale dataset for visual speech recognition. Preprint at arXiv (2018).

[3] Zhao, Y., Xu, R. & Song, M. A cascade sequence-to-sequence model for chinese mandarin lip reading. In Proceedings of the 1st ACM International Conference on Multimedia in Asia, pp. 1-6 (2019).

[4] Zadeh, A. B. et al. CMU-MOSEAS: A multimodal language dataset for spanish, portuguese, german and french. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 1801–1812 (2020).

[5] Cooke, M., Barker, J., Cunningham, S. & Shao, X. An audio-visual corpus for speech perceptionand automatic speech recognition. The Journal of the Acoustical Society of America, vol. 120, pp. 2421–2424 (2006).

[6] Alghamdi, N., Maddock, S., Marxer, R., Barker, J., & Brown, G. J. A corpus of audio-visual Lombard speech with frontal and profile views. The Journal of the Acoustical Society of America, vol. 143, pp. EL523-EL529 (2018).

[7] Harte, N. & Gillen, E. TCD-TIMIT: an audio-visual corpus of continuous speech. IEEE Transactions on Multimedia, vol. 17, pp. 603–615 (2015).
